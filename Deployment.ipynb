{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch 1.10.1 _CudaDeviceProperties(name='NVIDIA GeForce RTX 3070', major=8, minor=6, total_memory=8191MB, multi_processor_count=46)\n"
     ]
    }
   ],
   "source": [
    "import os, time\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(42)\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, classification_report\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(42)\n",
    "from torch import nn\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader, RandomSampler\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision.models import resnet\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "print('PyTorch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "else:\n",
    "    device = torch.device('CPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def Gen_dataset(images_path,  threads, mean, std, batch_size=1):\n",
    "    dataset = datasets.ImageFolder(root=images_path)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "images_path = 'C:/Users/mcafo/Downloads/archive/images'\n",
    "batch_size = 1\n",
    "threads = 18\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "dataset =  Gen_dataset(images_path, batch_size, threads, mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "image = Image.open(\"terrier.jpg\")\n",
    "transform = transforms.Compose([\n",
    "                                         transforms.Resize((224,224)),\n",
    "                                         transforms.ToTensor(),\n",
    "                                         transforms.Normalize(torch.Tensor(mean),\n",
    "                                                              torch.Tensor(std))])\n",
    "image = transform(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.hub import load_state_dict_from_url\n",
    "import torch\n",
    "\n",
    "from functools import partial\n",
    "from torch import nn, Tensor\n",
    "from torch.nn import functional as F\n",
    "from typing import Any, Callable, Dict, List, Optional, Sequence\n",
    "\n",
    "#from torchvision.models.utils import load_state_dict_from_url\n",
    "from torchvision.models.mobilenetv2 import _make_divisible, ConvBNActivation\n",
    "\n",
    "\n",
    "__all__ = [\"MobileNetV3\", \"mobilenet_v3_large\", \"mobilenet_v3_small\"]\n",
    "\n",
    "\n",
    "model_urls = {\n",
    "    \"mobilenet_v3_large\": \"https://download.pytorch.org/models/mobilenet_v3_large-8738ca79.pth\",\n",
    "    \"mobilenet_v3_small\": \"https://download.pytorch.org/models/mobilenet_v3_small-047dcff4.pth\",\n",
    "}\n",
    "\n",
    "\n",
    "class SqueezeExcitation(nn.Module):\n",
    "\n",
    "    def __init__(self, input_channels: int, squeeze_factor: int = 4):\n",
    "        super().__init__()\n",
    "        squeeze_channels = _make_divisible(input_channels // squeeze_factor, 8)\n",
    "        self.fc1 = nn.Conv2d(input_channels, squeeze_channels, 1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(squeeze_channels, input_channels, 1)\n",
    "\n",
    "    def _scale(self, input: Tensor, inplace: bool) -> Tensor:\n",
    "        scale = F.adaptive_avg_pool2d(input, 1)\n",
    "        scale = self.fc1(scale)\n",
    "        scale = self.relu(scale)\n",
    "        scale = self.fc2(scale)\n",
    "        return F.hardsigmoid(scale, inplace=inplace)\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        scale = self._scale(input, True)\n",
    "        return scale * input\n",
    "\n",
    "\n",
    "class InvertedResidualConfig:\n",
    "\n",
    "    def __init__(self, input_channels: int, kernel: int, expanded_channels: int, out_channels: int, use_se: bool,\n",
    "                 activation: str, stride: int, dilation: int, width_mult: float):\n",
    "        self.input_channels = self.adjust_channels(input_channels, width_mult)\n",
    "        self.kernel = kernel\n",
    "        self.expanded_channels = self.adjust_channels(expanded_channels, width_mult)\n",
    "        self.out_channels = self.adjust_channels(out_channels, width_mult)\n",
    "        self.use_se = use_se\n",
    "        self.use_hs = activation == \"HS\"\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "\n",
    "    @staticmethod\n",
    "    def adjust_channels(channels: int, width_mult: float):\n",
    "        return _make_divisible(channels * width_mult, 8)\n",
    "\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "\n",
    "    def __init__(self, cnf: InvertedResidualConfig, norm_layer: Callable[..., nn.Module],\n",
    "                 se_layer: Callable[..., nn.Module] = SqueezeExcitation):\n",
    "        super().__init__()\n",
    "        if not (1 <= cnf.stride <= 2):\n",
    "            raise ValueError('illegal stride value')\n",
    "\n",
    "        self.use_res_connect = cnf.stride == 1 and cnf.input_channels == cnf.out_channels\n",
    "\n",
    "        layers: List[nn.Module] = []\n",
    "        activation_layer = nn.Hardswish if cnf.use_hs else nn.ReLU\n",
    "\n",
    "        # expand\n",
    "        if cnf.expanded_channels != cnf.input_channels:\n",
    "            layers.append(ConvBNActivation(cnf.input_channels, cnf.expanded_channels, kernel_size=1,\n",
    "                                           norm_layer=norm_layer, activation_layer=activation_layer))\n",
    "\n",
    "        # depthwise\n",
    "        stride = 1 if cnf.dilation > 1 else cnf.stride\n",
    "        layers.append(ConvBNActivation(cnf.expanded_channels, cnf.expanded_channels, kernel_size=cnf.kernel,\n",
    "                                       stride=stride, dilation=cnf.dilation, groups=cnf.expanded_channels,\n",
    "                                       norm_layer=norm_layer, activation_layer=activation_layer))\n",
    "        if cnf.use_se:\n",
    "            layers.append(se_layer(cnf.expanded_channels))\n",
    "\n",
    "        # project\n",
    "        layers.append(ConvBNActivation(cnf.expanded_channels, cnf.out_channels, kernel_size=1, norm_layer=norm_layer,\n",
    "                                       activation_layer=nn.Identity))\n",
    "\n",
    "        self.block = nn.Sequential(*layers)\n",
    "        self.out_channels = cnf.out_channels\n",
    "        self._is_cn = cnf.stride > 1\n",
    "\n",
    "    def forward(self, input: Tensor) -> Tensor:\n",
    "        result = self.block(input)\n",
    "        if self.use_res_connect:\n",
    "            result += input\n",
    "        return result\n",
    "\n",
    "\n",
    "class MobileNetV3(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            inverted_residual_setting: List[InvertedResidualConfig],\n",
    "            last_channel: int,\n",
    "            num_classes: int = 120,\n",
    "            block: Optional[Callable[..., nn.Module]] = None,\n",
    "            norm_layer: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        MobileNet V3 main class\n",
    "\n",
    "        Args:\n",
    "            inverted_residual_setting (List[InvertedResidualConfig]): Network structure\n",
    "            last_channel (int): The number of channels on the penultimate layer\n",
    "            num_classes (int): Number of classes\n",
    "            block (Optional[Callable[..., nn.Module]]): Module specifying inverted residual building block for mobilenet\n",
    "            norm_layer (Optional[Callable[..., nn.Module]]): Module specifying the normalization layer to use\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        if not inverted_residual_setting:\n",
    "            raise ValueError(\"The inverted_residual_setting should not be empty\")\n",
    "        elif not (isinstance(inverted_residual_setting, Sequence) and\n",
    "                  all([isinstance(s, InvertedResidualConfig) for s in inverted_residual_setting])):\n",
    "            raise TypeError(\"The inverted_residual_setting should be List[InvertedResidualConfig]\")\n",
    "\n",
    "        if block is None:\n",
    "            block = InvertedResidual\n",
    "\n",
    "        if norm_layer is None:\n",
    "            norm_layer = partial(nn.BatchNorm2d, eps=0.001, momentum=0.01)\n",
    "\n",
    "        layers: List[nn.Module] = []\n",
    "\n",
    "        # building first layer\n",
    "        firstconv_output_channels = inverted_residual_setting[0].input_channels\n",
    "        layers.append(ConvBNActivation(3, firstconv_output_channels, kernel_size=3, stride=2, norm_layer=norm_layer,\n",
    "                                       activation_layer=nn.Hardswish))\n",
    "\n",
    "        # building inverted residual blocks\n",
    "        for cnf in inverted_residual_setting:\n",
    "            layers.append(block(cnf, norm_layer))\n",
    "\n",
    "        # building last several layers\n",
    "        lastconv_input_channels = inverted_residual_setting[-1].out_channels\n",
    "        lastconv_output_channels = 6 * lastconv_input_channels\n",
    "        layers.append(ConvBNActivation(lastconv_input_channels, lastconv_output_channels, kernel_size=1,\n",
    "                                       norm_layer=norm_layer, activation_layer=nn.Hardswish))\n",
    "\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(lastconv_output_channels, last_channel),\n",
    "            nn.Hardswish(inplace=True),\n",
    "            nn.Dropout(p=0.2, inplace=True),\n",
    "            nn.Linear(last_channel, num_classes),\n",
    "            #added by me\n",
    "            #nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        x = self.features(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "def _mobilenet_v3_conf(arch: str, params: Dict[str, Any]):\n",
    "    # non-public config parameters\n",
    "    reduce_divider = 2 if params.pop('_reduced_tail', False) else 1\n",
    "    dilation = 2 if params.pop('_dilated', False) else 1\n",
    "    width_mult = params.pop('_width_mult', 1.0)\n",
    "\n",
    "    bneck_conf = partial(InvertedResidualConfig, width_mult=width_mult)\n",
    "    adjust_channels = partial(InvertedResidualConfig.adjust_channels, width_mult=width_mult)\n",
    "\n",
    "    if arch == \"mobilenet_v3_large\":\n",
    "        inverted_residual_setting = [\n",
    "            bneck_conf(16, 3, 16, 16, False, \"RE\", 1, 1),\n",
    "            bneck_conf(16, 3, 64, 24, False, \"RE\", 2, 1),  # C1\n",
    "            bneck_conf(24, 3, 72, 24, False, \"RE\", 1, 1),\n",
    "            bneck_conf(24, 5, 72, 40, True, \"RE\", 2, 1),  # C2\n",
    "            bneck_conf(40, 5, 120, 40, True, \"RE\", 1, 1),\n",
    "            bneck_conf(40, 5, 120, 40, True, \"RE\", 1, 1),\n",
    "            bneck_conf(40, 3, 240, 80, False, \"HS\", 2, 1),  # C3\n",
    "            bneck_conf(80, 3, 200, 80, False, \"HS\", 1, 1),\n",
    "            bneck_conf(80, 3, 184, 80, False, \"HS\", 1, 1),\n",
    "            bneck_conf(80, 3, 184, 80, False, \"HS\", 1, 1),\n",
    "            bneck_conf(80, 3, 480, 112, True, \"HS\", 1, 1),\n",
    "            bneck_conf(112, 3, 672, 112, True, \"HS\", 1, 1),\n",
    "            bneck_conf(112, 5, 672, 160 // reduce_divider, True, \"HS\", 2, dilation),  # C4\n",
    "            bneck_conf(160 // reduce_divider, 5, 960 // reduce_divider, 160 // reduce_divider, True, \"HS\", 1, dilation),\n",
    "            bneck_conf(160 // reduce_divider, 5, 960 // reduce_divider, 160 // reduce_divider, True, \"HS\", 1, dilation),\n",
    "        ]\n",
    "        last_channel = adjust_channels(1280 // reduce_divider)  # C5\n",
    "    elif arch == \"mobilenet_v3_small\":\n",
    "        inverted_residual_setting = [\n",
    "            bneck_conf(16, 3, 16, 16, True, \"RE\", 2, 1),  # C1\n",
    "            bneck_conf(16, 3, 72, 24, False, \"RE\", 2, 1),  # C2\n",
    "            bneck_conf(24, 3, 88, 24, False, \"RE\", 1, 1),\n",
    "            bneck_conf(24, 5, 96, 40, True, \"HS\", 2, 1),  # C3\n",
    "            bneck_conf(40, 5, 240, 40, True, \"HS\", 1, 1),\n",
    "            bneck_conf(40, 5, 240, 40, True, \"HS\", 1, 1),\n",
    "            bneck_conf(40, 5, 120, 48, True, \"HS\", 1, 1),\n",
    "            bneck_conf(48, 5, 144, 48, True, \"HS\", 1, 1),\n",
    "            bneck_conf(48, 5, 288, 96 // reduce_divider, True, \"HS\", 2, dilation),  # C4\n",
    "            bneck_conf(96 // reduce_divider, 5, 576 // reduce_divider, 96 // reduce_divider, True, \"HS\", 1, dilation),\n",
    "            bneck_conf(96 // reduce_divider, 5, 576 // reduce_divider, 96 // reduce_divider, True, \"HS\", 1, dilation),\n",
    "        ]\n",
    "        last_channel = adjust_channels(1024 // reduce_divider)  # C5\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type {}\".format(arch))\n",
    "\n",
    "    return inverted_residual_setting, last_channel\n",
    "\n",
    "\n",
    "def _mobilenet_v3_model(\n",
    "    arch: str,\n",
    "    inverted_residual_setting: List[InvertedResidualConfig],\n",
    "    last_channel: int,\n",
    "    pretrained: bool,\n",
    "    progress: bool,\n",
    "    **kwargs: Any\n",
    "):\n",
    "    model = MobileNetV3(inverted_residual_setting, last_channel, **kwargs)\n",
    "    if pretrained:\n",
    "        if model_urls.get(arch, None) is None:\n",
    "            raise ValueError(\"No checkpoint is available for model type {}\".format(arch))\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch], progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def mobilenet_v3_large(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> MobileNetV3:\n",
    "    \"\"\"\n",
    "    Constructs a large MobileNetV3 architecture from\n",
    "    `\"Searching for MobileNetV3\" <https://arxiv.org/abs/1905.02244>`_.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    arch = \"mobilenet_v3_large\"\n",
    "    inverted_residual_setting, last_channel = _mobilenet_v3_conf(arch, kwargs)\n",
    "    return _mobilenet_v3_model(arch, inverted_residual_setting, last_channel, pretrained, progress, **kwargs)\n",
    "\n",
    "\n",
    "\n",
    "def mobilenet_v3_small(pretrained: bool = False, progress: bool = True, **kwargs: Any) -> MobileNetV3:\n",
    "    \"\"\"\n",
    "    Constructs a small MobileNetV3 architecture from\n",
    "    `\"Searching for MobileNetV3\" <https://arxiv.org/abs/1905.02244>`_.\n",
    "\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    arch = \"mobilenet_v3_small\"\n",
    "    inverted_residual_setting, last_channel = _mobilenet_v3_conf(arch, kwargs)\n",
    "    return _mobilenet_v3_model(arch, inverted_residual_setting, last_channel, pretrained, progress, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\torchvision\\models\\mobilenetv2.py:24: FutureWarning: The ConvBNReLU/ConvBNActivation classes are deprecated and will be removed in future versions. Use torchvision.ops.misc.ConvNormActivation instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = mobilenet_v3_large(pretrained=False).to(device)\n",
    "model.load_state_dict(torch.load('C:/Users/mcafo/Downloads/archive/80ModelLarge.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class_names = dataset.classes\n",
    "class_names = [classes for classes in class_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-1051c9d358cf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# print(img.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mbreed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mbreed\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbreed\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbreed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"-\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbreed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbreed\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "image = image.to(device)\n",
    "# print(img.shape)\n",
    "output = model(torch.unsqueeze(image, 0))\n",
    "breed = class_names[torch.argmax(output)]\n",
    "breed = breed[breed.index(\"-\") + 1: len(breed)]\n",
    "print(breed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import coremltools as ct\n",
    "# Trace the model with random data.\n",
    "\n",
    "example_input =  torch.rand(1, 3, 224, 224).to(device)\n",
    "traced_model = torch.jit.trace(model, example_input)\n",
    "out = traced_model(example_input)\n",
    "\n",
    "# Using image_input in the inputs parameter:\n",
    "# Convert to Core ML using the Unified Conversion API.\n",
    "converted_model = ct.convert(\n",
    "    traced_model,\n",
    "    inputs=[ct.TensorType(shape=example_input.shape)]\n",
    " )\n",
    "\n",
    "# Save the converted model.\n",
    "converted_model.save(\"mobilenet.mlmodel\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
